{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c64b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import os\n",
    "from src.slm import TinyGPT\n",
    "from src.utils.dataset import QADataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ad30d",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb45de83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name_traing = \"C:\\\\Users\\\\thomm\\\\OneDrive\\\\Desktop\\\\Repositorios\\\\conecta2ai\\\\TinyGPT-SLM\\data\\\\training_dataset.json\"\n",
    "file_name_test = \"C:\\\\Users\\\\thomm\\\\OneDrive\\\\Desktop\\\\Repositorios\\\\conecta2ai\\\\TinyGPT-SLM\\data\\\\test_dataset.json\" \n",
    "synthetic_dataset = QADataset().load_jsonl_file(file_name_traing)\n",
    "synthetic_dataset_test = QADataset().load_jsonl_file(file_name_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6046e48",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e078f872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Vocabulary built from Q&A with 445 tokens.\n",
      "[INFO] Context length: 22\n",
      "[INFO] Max original sequence length found: 20\n",
      "[INFO] Sequences adjusted to max_length: 22\n",
      "[INFO] Using CPU.\n",
      "==================================================\n",
      "                  TRAINING MODEL                  \n",
      "==================================================\n",
      "[INFO] Epoch 1/100 - Loss: 5.4949\n",
      "[INFO] Epoch 2/100 - Loss: 4.5723\n",
      "[INFO] Epoch 3/100 - Loss: 4.1892\n",
      "[INFO] Epoch 4/100 - Loss: 3.8504\n",
      "[INFO] Epoch 5/100 - Loss: 3.4788\n",
      "[INFO] Epoch 6/100 - Loss: 3.1366\n",
      "[INFO] Epoch 7/100 - Loss: 2.7862\n",
      "[INFO] Epoch 8/100 - Loss: 2.4214\n",
      "[INFO] Epoch 9/100 - Loss: 2.1405\n",
      "[INFO] Epoch 10/100 - Loss: 1.8677\n",
      "[INFO] Epoch 11/100 - Loss: 1.6534\n",
      "[INFO] Epoch 12/100 - Loss: 1.4484\n",
      "[INFO] Epoch 13/100 - Loss: 1.3036\n",
      "[INFO] Epoch 14/100 - Loss: 1.1923\n",
      "[INFO] Epoch 15/100 - Loss: 1.0597\n",
      "[INFO] Epoch 16/100 - Loss: 1.0108\n",
      "[INFO] Epoch 17/100 - Loss: 0.9044\n",
      "[INFO] Epoch 18/100 - Loss: 0.8579\n",
      "[INFO] Epoch 19/100 - Loss: 0.8873\n",
      "[INFO] Epoch 20/100 - Loss: 0.7978\n",
      "[INFO] Epoch 21/100 - Loss: 0.7146\n",
      "[INFO] Epoch 22/100 - Loss: 0.6863\n",
      "[INFO] Epoch 23/100 - Loss: 0.6751\n",
      "[INFO] Epoch 24/100 - Loss: 0.6525\n",
      "[INFO] Epoch 25/100 - Loss: 0.6332\n",
      "[INFO] Epoch 26/100 - Loss: 0.6419\n",
      "[INFO] Epoch 27/100 - Loss: 0.6200\n",
      "[INFO] Epoch 28/100 - Loss: 0.6542\n",
      "[INFO] Epoch 29/100 - Loss: 0.5923\n",
      "[INFO] Epoch 30/100 - Loss: 0.5776\n",
      "[INFO] Epoch 31/100 - Loss: 0.5810\n",
      "[INFO] Epoch 32/100 - Loss: 0.5519\n",
      "[INFO] Epoch 33/100 - Loss: 0.5498\n",
      "[INFO] Epoch 34/100 - Loss: 0.5527\n",
      "[INFO] Epoch 35/100 - Loss: 0.5564\n",
      "[INFO] Epoch 36/100 - Loss: 0.5800\n",
      "[INFO] Epoch 37/100 - Loss: 0.6156\n",
      "[INFO] Epoch 38/100 - Loss: 0.5753\n",
      "[INFO] Epoch 39/100 - Loss: 0.5591\n",
      "[INFO] Epoch 40/100 - Loss: 0.5858\n",
      "[INFO] Epoch 41/100 - Loss: 0.5881\n",
      "[INFO] Epoch 42/100 - Loss: 0.5730\n",
      "[INFO] Epoch 43/100 - Loss: 0.5408\n",
      "[INFO] Epoch 44/100 - Loss: 0.5517\n",
      "[INFO] Epoch 45/100 - Loss: 0.5671\n",
      "[INFO] Epoch 46/100 - Loss: 0.5861\n",
      "[INFO] Epoch 47/100 - Loss: 0.5766\n",
      "[INFO] Epoch 48/100 - Loss: 0.5511\n",
      "[INFO] Epoch 49/100 - Loss: 0.5012\n",
      "[INFO] Epoch 50/100 - Loss: 0.5268\n",
      "[INFO] Epoch 51/100 - Loss: 0.5144\n",
      "[INFO] Epoch 52/100 - Loss: 0.5201\n",
      "[INFO] Epoch 53/100 - Loss: 0.5030\n",
      "[INFO] Epoch 54/100 - Loss: 0.4880\n",
      "[INFO] Epoch 55/100 - Loss: 0.5327\n",
      "[INFO] Epoch 56/100 - Loss: 0.5386\n",
      "[INFO] Epoch 57/100 - Loss: 0.4890\n",
      "[INFO] Epoch 58/100 - Loss: 0.4904\n",
      "[INFO] Epoch 59/100 - Loss: 0.4876\n",
      "[INFO] Epoch 60/100 - Loss: 0.4850\n",
      "[INFO] Epoch 61/100 - Loss: 0.5045\n",
      "[INFO] Epoch 62/100 - Loss: 0.5136\n",
      "[INFO] Epoch 63/100 - Loss: 0.5172\n",
      "[INFO] Epoch 64/100 - Loss: 0.5095\n",
      "[INFO] Epoch 65/100 - Loss: 0.5350\n",
      "[INFO] Epoch 66/100 - Loss: 0.5566\n",
      "[INFO] Epoch 67/100 - Loss: 0.5383\n",
      "[INFO] Epoch 68/100 - Loss: 0.5300\n",
      "[INFO] Epoch 69/100 - Loss: 0.5038\n",
      "[INFO] Epoch 70/100 - Loss: 0.4915\n",
      "[INFO] Epoch 71/100 - Loss: 0.4842\n",
      "[INFO] Epoch 72/100 - Loss: 0.4792\n",
      "[INFO] Epoch 73/100 - Loss: 0.4464\n",
      "[INFO] Epoch 74/100 - Loss: 0.4568\n",
      "[INFO] Epoch 75/100 - Loss: 0.4643\n",
      "[INFO] Epoch 76/100 - Loss: 0.4677\n",
      "[INFO] Epoch 77/100 - Loss: 0.5158\n",
      "[INFO] Epoch 78/100 - Loss: 0.4841\n",
      "[INFO] Epoch 79/100 - Loss: 0.4852\n",
      "[INFO] Epoch 80/100 - Loss: 0.5031\n",
      "[INFO] Epoch 81/100 - Loss: 0.5185\n",
      "[INFO] Epoch 82/100 - Loss: 0.5437\n",
      "[INFO] Epoch 83/100 - Loss: 0.5436\n",
      "[INFO] Epoch 84/100 - Loss: 0.4961\n",
      "[INFO] Epoch 85/100 - Loss: 0.4877\n",
      "[INFO] Epoch 86/100 - Loss: 0.4653\n",
      "[INFO] Epoch 87/100 - Loss: 0.4692\n",
      "[INFO] Epoch 88/100 - Loss: 0.4430\n",
      "[INFO] Epoch 89/100 - Loss: 0.4427\n",
      "[INFO] Epoch 90/100 - Loss: 0.4476\n",
      "[INFO] Epoch 91/100 - Loss: 0.4406\n",
      "[INFO] Epoch 92/100 - Loss: 0.4438\n",
      "[INFO] Epoch 93/100 - Loss: 0.4385\n",
      "[INFO] Epoch 94/100 - Loss: 0.4433\n",
      "[INFO] Epoch 95/100 - Loss: 0.4567\n",
      "[INFO] Epoch 96/100 - Loss: 0.4647\n",
      "[INFO] Epoch 97/100 - Loss: 0.4682\n",
      "[INFO] Epoch 98/100 - Loss: 0.4920\n",
      "[INFO] Epoch 99/100 - Loss: 0.4704\n",
      "[INFO] Epoch 100/100 - Loss: 0.4905\n",
      "==================================================\n",
      "                TRAINING COMPLETE                 \n",
      "==================================================\n",
      "[INFO] Config successfully saved to: 'output_dir\\config.json'\n",
      "[INFO] Vocabulary successfully saved with 445 tokens.\n",
      "[INFO] Model, configuration, and tokenizer successfully saved in 'output_dir'\n",
      "[INFO] Compatible Arduino code generated at C:\\Users\\thomm\\OneDrive\\Documents\\Arduino\\transformers/gpt_model.h and C:\\Users\\thomm\\OneDrive\\Documents\\Arduino\\transformers/gpt_model.cpp\n",
      "[INFO] Check the generated files at: C:\\Users\\thomm\\OneDrive\\Documents\\Arduino\\transformers\n"
     ]
    }
   ],
   "source": [
    "model = TinyGPT(\n",
    "    embedding_dim=16,\n",
    "    heads=8,\n",
    "    layers=4,\n",
    "    drop_rate = 0.01)\n",
    "\n",
    "model.train(\n",
    "    synthetic_dataset=synthetic_dataset,\n",
    "    num_epochs=100,\n",
    "    batch_size=4,\n",
    "    lr=0.01,\n",
    "    optimizer_cls=AdamW,\n",
    "    loss_fn_cls=CrossEntropyLoss\n",
    ")\n",
    "\n",
    "model.save(output_folder=\"output_dir\")\n",
    "model.code_generator(model_folder = \"output_dir\",\n",
    "                     output_arduino_folder=\"C:\\\\Users\\\\thomm\\\\OneDrive\\\\Documents\\\\Arduino\\\\transformers\")\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0be825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'be aware of potential hazards'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(input_question = \"Familiarize yourself with your vehicle's functions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d792b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Max original sequence length found: 22\n",
      "[INFO] Sequences adjusted to max_length: 22\n",
      "[INFO] Using CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_loss': 8.586600303649902,\n",
       " 'total_items': 1,\n",
       " 'correct_predictions': 94,\n",
       " 'total_tokens': 436,\n",
       " 'exact_matches': 0,\n",
       " 'bleu_scores': [],\n",
       " 'generated_examples': [],\n",
       " 'avg_loss': 8.586600303649902,\n",
       " 'perplexity': 5359.362463694249,\n",
       " 'token_accuracy': 0.21559633027522937,\n",
       " 'num_samples': 30}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluation(synthetic_dataset_test, batch_size=32, num_examples = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
